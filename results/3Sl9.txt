sbc-bench v0.9.3 zqykj-K681-F223 (Wed, 16 Mar 2022 00:19:38 +0800)

Distributor ID:	Kylin
Description:	Kylin V10 SP1
Release:	v10
Codename:	kylin
Architecture:	arm64

Device Info:
	Manufacturer: Huanghe
	Product Name: K681 F223
	Version: V1.0
	SKU Number: 80180113
	Family: N/A

BIOS/UEFI:
	Vendor: ZD-TECH
	Version: KL4.27.D.040.D
	Release Date: 06/28/2021
	BIOS Revision: 4.0
	Firmware Revision: 0.3

/usr/bin/gcc (Ubuntu 9.3.0-10kylin2) 9.3.0

Uptime: 00:19:38 up 15 days,  6:30,  4 users,  load average: 3.17, 3.07, 3.06

Linux 5.4.18-35-generic (zqykj-K681-F223) 	03/16/22 	_aarch64_	(8 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           4.31    0.00    3.06    1.74    0.00   90.90

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
nvme0n1          55.75       413.10       423.33       141.34  545059279  558554884  186493032
sda               0.00         0.01         0.00         0.00       8163          0          0

              total        used        free      shared  buff/cache   available
Mem:           15Gi       1.9Gi        13Gi        39Mi       571Mi        13Gi
Swap:          18Gi       1.0Gi        17Gi

Filename				Type		Size	Used	Priority
/dev/nvme0n1p6                         	partition	19511292	1096192	-2

##########################################################################

Checking cpufreq OPP for cpu0-cpu1 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 2300 (2298.431/2298.798/2297.149)
Cpufreq OPP: 1150    Measured: 1150 (1148.232/843.655/1148.653)
Cpufreq OPP:  766    Measured:  770 (765.341/765.270/765.076)
Cpufreq OPP:  575    Measured:  575 (573.328/573.832/573.763)

Checking cpufreq OPP for cpu2-cpu3 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 2300 (2298.562/2298.562/2298.562)
Cpufreq OPP: 1150    Measured: 1150 (1148.947/1148.871/1148.730)
Cpufreq OPP:  766    Measured:  770 (765.439/765.527/765.288)
Cpufreq OPP:  575    Measured:  575 (573.900/573.576/573.838)

Checking cpufreq OPP for cpu4-cpu5 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 2300 (2298.431/2298.588/2298.405)
Cpufreq OPP: 1150    Measured: 1150 (1148.743/1148.551/1148.820)
Cpufreq OPP:  766    Measured:  770 (765.510/765.217/765.448)
Cpufreq OPP:  575    Measured:  575 (573.688/573.607/573.844)

Checking cpufreq OPP for cpu6-cpu7 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 2300 (2298.746/2298.641/2298.379)
Cpufreq OPP: 1150    Measured: 1150 (1148.883/1148.794/1148.666)
Cpufreq OPP:  766    Measured:  770 (765.492/765.244/765.350)
Cpufreq OPP:  575    Measured:  575 (573.875/573.657/573.844)

##########################################################################

Hardware sensors:

scpi_sensors-isa-0000
:             +52.3 C  
:             +46.0 C  

amdgpu-pci-0400
fan1:           0 RPM
edge:         +56.0 C  (crit = +120.0 C, hyst = +90.0 C)

##########################################################################

Executing benchmark on cpu0 (Phytium FTC663):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :   4007.6 MB/s (0.1%)
 C copy backwards (32 byte blocks)                    :   4009.8 MB/s
 C copy backwards (64 byte blocks)                    :   3999.6 MB/s
 C copy                                               :   4015.3 MB/s (1.9%)
 C copy prefetched (32 bytes step)                    :   4192.2 MB/s
 C copy prefetched (64 bytes step)                    :   4187.3 MB/s
 C 2-pass copy                                        :   3462.4 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   3516.6 MB/s (1.6%)
 C 2-pass copy prefetched (64 bytes step)             :   3544.5 MB/s
 C fill                                               :  16282.3 MB/s (1.8%)
 C fill (shuffle within 16 byte blocks)               :  16294.1 MB/s
 C fill (shuffle within 32 byte blocks)               :  16287.2 MB/s
 C fill (shuffle within 64 byte blocks)               :  16301.7 MB/s
 ---
 standard memcpy                                      :   4065.2 MB/s
 standard memset                                      :  16265.2 MB/s (1.9%)
 ---
 NEON LDP/STP copy                                    :   4021.7 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :   1891.5 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :   1890.6 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :   4306.2 MB/s (1.9%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :   4303.4 MB/s (1.0%)
 NEON LD1/ST1 copy                                    :   4044.8 MB/s (1.9%)
 NEON STP fill                                        :  16290.0 MB/s (0.4%)
 NEON STNP fill                                       :  16287.5 MB/s (1.7%)
 ARM LDP/STP copy                                     :   4021.3 MB/s
 ARM STP fill                                         :  16275.9 MB/s (1.8%)
 ARM STNP fill                                        :  16276.1 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    106.5 MB/s
 NEON LDP/STP 2-pass copy (from framebuffer)          :    105.1 MB/s
 NEON LD1/ST1 copy (from framebuffer)                 :    181.1 MB/s
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    178.0 MB/s
 ARM LDP/STP copy (from framebuffer)                  :     58.5 MB/s (5.6%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :     50.2 MB/s

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    1.0 ns          /     1.8 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    8.4 ns          /    10.7 ns 
    524288 :    9.8 ns          /    12.2 ns 
   1048576 :   10.5 ns          /    13.1 ns 
   2097152 :   12.8 ns          /    16.3 ns 
   4194304 :   22.4 ns          /    28.3 ns 
   8388608 :   69.7 ns          /   109.6 ns 
  16777216 :  119.8 ns          /   168.3 ns 
  33554432 :  144.9 ns          /   189.2 ns 
  67108864 :  163.6 ns          /   209.5 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    6.9 ns          /     8.8 ns 
    524288 :    7.5 ns          /     9.1 ns 
   1048576 :    7.9 ns          /     9.2 ns 
   2097152 :    9.0 ns          /    10.4 ns 
   4194304 :   19.3 ns          /    24.0 ns 
   8388608 :   59.1 ns          /    88.9 ns 
  16777216 :  104.0 ns          /   141.1 ns 
  33554432 :  130.4 ns          /   158.7 ns 
  67108864 :  142.3 ns          /   164.7 ns 

Executing benchmark on cpu2 (Phytium FTC663):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :   3921.2 MB/s (0.3%)
 C copy backwards (32 byte blocks)                    :   3996.6 MB/s (1.4%)
 C copy backwards (64 byte blocks)                    :   3986.4 MB/s
 C copy                                               :   4002.4 MB/s
 C copy prefetched (32 bytes step)                    :   4178.7 MB/s
 C copy prefetched (64 bytes step)                    :   4175.3 MB/s
 C 2-pass copy                                        :   3452.2 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   3509.3 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   3535.1 MB/s
 C fill                                               :  16248.3 MB/s (0.4%)
 C fill (shuffle within 16 byte blocks)               :  16259.6 MB/s
 C fill (shuffle within 32 byte blocks)               :  16266.9 MB/s
 C fill (shuffle within 64 byte blocks)               :  16271.0 MB/s
 ---
 standard memcpy                                      :   4054.6 MB/s
 standard memset                                      :  16242.5 MB/s (0.4%)
 ---
 NEON LDP/STP copy                                    :   4008.7 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :   1888.3 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :   1887.2 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :   4304.8 MB/s
 NEON LDP/STP copy pldl1keep (64 bytes step)          :   4300.9 MB/s
 NEON LD1/ST1 copy                                    :   4047.3 MB/s
 NEON STP fill                                        :  16315.6 MB/s (0.4%)
 NEON STNP fill                                       :  16306.8 MB/s (0.5%)
 ARM LDP/STP copy                                     :   4026.9 MB/s
 ARM STP fill                                         :  16240.4 MB/s (0.9%)
 ARM STNP fill                                        :  16263.2 MB/s (0.4%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    106.7 MB/s
 NEON LDP/STP 2-pass copy (from framebuffer)          :    105.8 MB/s
 NEON LD1/ST1 copy (from framebuffer)                 :    181.1 MB/s
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    179.2 MB/s (0.2%)
 ARM LDP/STP copy (from framebuffer)                  :     58.7 MB/s
 ARM LDP/STP 2-pass copy (from framebuffer)           :     58.4 MB/s (0.1%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    8.4 ns          /    10.7 ns 
    524288 :    9.7 ns          /    12.2 ns 
   1048576 :   10.5 ns          /    13.0 ns 
   2097152 :   11.7 ns          /    14.5 ns 
   4194304 :   22.0 ns          /    27.7 ns 
   8388608 :   69.4 ns          /   107.7 ns 
  16777216 :  118.8 ns          /   167.5 ns 
  33554432 :  144.9 ns          /   189.0 ns 
  67108864 :  163.4 ns          /   209.2 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    6.9 ns          /     8.8 ns 
    524288 :    7.5 ns          /     9.1 ns 
   1048576 :    7.9 ns          /     9.2 ns 
   2097152 :    8.8 ns          /    10.2 ns 
   4194304 :   17.7 ns          /    23.7 ns 
   8388608 :   58.8 ns          /    88.7 ns 
  16777216 :  104.3 ns          /   141.0 ns 
  33554432 :  127.8 ns          /   157.7 ns 
  67108864 :  140.9 ns          /   164.3 ns 

Executing benchmark on cpu4 (Phytium FTC663):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :   4031.4 MB/s
 C copy backwards (32 byte blocks)                    :   4031.2 MB/s
 C copy backwards (64 byte blocks)                    :   4024.6 MB/s
 C copy                                               :   4039.9 MB/s
 C copy prefetched (32 bytes step)                    :   4217.0 MB/s
 C copy prefetched (64 bytes step)                    :   4215.0 MB/s
 C 2-pass copy                                        :   3470.5 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   3523.6 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   3554.1 MB/s
 C fill                                               :  16286.7 MB/s (0.4%)
 C fill (shuffle within 16 byte blocks)               :  16304.9 MB/s
 C fill (shuffle within 32 byte blocks)               :  16312.2 MB/s
 C fill (shuffle within 64 byte blocks)               :  16317.7 MB/s
 ---
 standard memcpy                                      :   4093.4 MB/s
 standard memset                                      :  16314.3 MB/s (0.4%)
 ---
 NEON LDP/STP copy                                    :   4049.9 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :   1896.6 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :   1895.9 MB/s (0.6%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :   4329.0 MB/s
 NEON LDP/STP copy pldl1keep (64 bytes step)          :   4325.9 MB/s
 NEON LD1/ST1 copy                                    :   4071.3 MB/s (0.2%)
 NEON STP fill                                        :  16280.4 MB/s (0.4%)
 NEON STNP fill                                       :  16290.7 MB/s (0.3%)
 ARM LDP/STP copy                                     :   4047.6 MB/s
 ARM STP fill                                         :  16277.9 MB/s (0.4%)
 ARM STNP fill                                        :  16261.8 MB/s (0.3%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    107.4 MB/s
 NEON LDP/STP 2-pass copy (from framebuffer)          :    106.6 MB/s
 NEON LD1/ST1 copy (from framebuffer)                 :    182.1 MB/s
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    180.2 MB/s
 ARM LDP/STP copy (from framebuffer)                  :     59.2 MB/s
 ARM LDP/STP 2-pass copy (from framebuffer)           :     58.9 MB/s (0.3%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    8.4 ns          /    10.7 ns 
    524288 :    9.7 ns          /    12.2 ns 
   1048576 :   10.5 ns          /    13.1 ns 
   2097152 :   11.8 ns          /    14.5 ns 
   4194304 :   20.9 ns          /    27.4 ns 
   8388608 :   68.9 ns          /   107.7 ns 
  16777216 :  118.6 ns          /   167.3 ns 
  33554432 :  144.2 ns          /   188.5 ns 
  67108864 :  163.0 ns          /   208.9 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    6.9 ns          /     8.8 ns 
    524288 :    7.5 ns          /     9.1 ns 
   1048576 :    7.8 ns          /     9.2 ns 
   2097152 :    8.6 ns          /    10.6 ns 
   4194304 :   18.8 ns          /    23.5 ns 
   8388608 :   58.0 ns          /    88.0 ns 
  16777216 :  103.4 ns          /   140.5 ns 
  33554432 :  127.2 ns          /   157.2 ns 
  67108864 :  140.3 ns          /   163.8 ns 

Executing benchmark on cpu6 (Phytium FTC663):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :   4025.1 MB/s
 C copy backwards (32 byte blocks)                    :   4023.5 MB/s
 C copy backwards (64 byte blocks)                    :   4016.5 MB/s
 C copy                                               :   4031.2 MB/s
 C copy prefetched (32 bytes step)                    :   4209.8 MB/s
 C copy prefetched (64 bytes step)                    :   4207.6 MB/s
 C 2-pass copy                                        :   3463.2 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   3520.7 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   3552.9 MB/s
 C fill                                               :  16308.3 MB/s (0.4%)
 C fill (shuffle within 16 byte blocks)               :  16316.0 MB/s
 C fill (shuffle within 32 byte blocks)               :  16312.7 MB/s
 C fill (shuffle within 64 byte blocks)               :  16307.5 MB/s (0.9%)
 ---
 standard memcpy                                      :   4085.4 MB/s
 standard memset                                      :  16293.7 MB/s (0.4%)
 ---
 NEON LDP/STP copy                                    :   4042.9 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :   1897.3 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :   1896.2 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :   4328.5 MB/s
 NEON LDP/STP copy pldl1keep (64 bytes step)          :   4325.1 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :   4068.4 MB/s
 NEON STP fill                                        :  16300.4 MB/s (0.4%)
 NEON STNP fill                                       :  16298.6 MB/s
 ARM LDP/STP copy                                     :   4039.9 MB/s
 ARM STP fill                                         :  16277.1 MB/s (0.3%)
 ARM STNP fill                                        :  16313.7 MB/s (0.2%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    107.0 MB/s
 NEON LDP/STP 2-pass copy (from framebuffer)          :    106.3 MB/s
 NEON LD1/ST1 copy (from framebuffer)                 :    181.5 MB/s
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    179.7 MB/s
 ARM LDP/STP copy (from framebuffer)                  :     59.1 MB/s (0.3%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :     58.5 MB/s

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    8.4 ns          /    10.7 ns 
    524288 :    9.7 ns          /    12.2 ns 
   1048576 :   10.5 ns          /    13.1 ns 
   2097152 :   11.6 ns          /    14.2 ns 
   4194304 :   20.6 ns          /    27.4 ns 
   8388608 :   69.5 ns          /   107.7 ns 
  16777216 :  118.9 ns          /   167.5 ns 
  33554432 :  144.4 ns          /   188.7 ns 
  67108864 :  163.2 ns          /   209.0 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    3.9 ns          /     6.1 ns 
    131072 :    5.9 ns          /     8.1 ns 
    262144 :    6.9 ns          /     8.8 ns 
    524288 :    7.5 ns          /     9.1 ns 
   1048576 :    7.7 ns          /     9.2 ns 
   2097152 :    8.7 ns          /    10.1 ns 
   4194304 :   18.5 ns          /    23.4 ns 
   8388608 :   57.4 ns          /    88.6 ns 
  16777216 :  103.6 ns          /   140.8 ns 
  33554432 :  127.4 ns          /   157.2 ns 
  67108864 :  140.5 ns          /   163.8 ns 

##########################################################################

Executing benchmark on each cluster individually

OpenSSL 1.1.1f, built on 31 Mar 2020
type             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes
aes-128-cbc     437777.65k   829081.62k  1056241.58k  1140366.68k  1173408.43k  1174896.64k
aes-128-cbc     437831.45k   829154.54k  1067683.67k  1140411.73k  1175655.77k  1178086.06k
aes-128-cbc     437828.92k   829156.67k  1067722.15k  1140431.87k  1175693.99k  1176993.79k
aes-128-cbc     437830.87k   829168.98k  1067735.21k  1140444.84k  1175707.65k  1178195.29k
aes-192-cbc     408586.66k   725312.15k   890104.41k   950253.57k   974479.36k   976196.95k
aes-192-cbc     408598.73k   725299.99k   899780.18k   950353.24k   974558.55k   974946.30k
aes-192-cbc     408598.38k   725286.19k   899791.53k   950371.67k   974566.74k   976224.26k
aes-192-cbc     408636.47k   725375.23k   899751.34k   950351.19k   974266.37k   975415.98k
aes-256-cbc     378564.02k   639575.72k   764538.03k   811513.17k   826630.14k   828063.74k
aes-256-cbc     378596.79k   639633.54k   772839.85k   810242.05k   827875.33k   828582.57k
aes-256-cbc     378601.93k   639611.54k   772860.25k   810261.50k   827872.60k   829057.71k
aes-256-cbc     378593.35k   639626.52k   772836.95k   810223.27k   826378.92k   828358.66k

##########################################################################

Executing benchmark single-threaded on cpu0 (Phytium FTC663)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - 64000000 - - - - - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       2408   100   2343   2343  |      28526   100   2447   2436
23:       2280   100   2323   2323  |      28184   100   2451   2440
24:       2195   100   2371   2360  |      27876   100   2447   2447
25:       2134   100   2447   2437  |      27454   100   2444   2444
----------------------------------  | ------------------------------
Avr:             100   2371   2366  |              100   2447   2442
Tot:             100   2409   2404

Executing benchmark single-threaded on cpu2 (Phytium FTC663)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - - 64000000 - - - - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       2431   100   2365   2365  |      28637   100   2445   2445
23:       2291   100   2335   2335  |      28285   100   2448   2448
24:       2206   100   2372   2372  |      27829   100   2443   2443
25:       2140   100   2444   2444  |      27468   100   2445   2445
----------------------------------  | ------------------------------
Avr:             100   2379   2379  |              100   2445   2445
Tot:             100   2412   2412

Executing benchmark single-threaded on cpu4 (Phytium FTC663)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       2413   100   2348   2348  |      28619   100   2444   2444
23:       2283   100   2326   2326  |      28271   100   2447   2447
24:       2208   100   2375   2375  |      27836   100   2444   2444
25:       2146   100   2450   2450  |      27444   100   2443   2443
----------------------------------  | ------------------------------
Avr:             100   2375   2375  |              100   2444   2444
Tot:             100   2410   2410

Executing benchmark single-threaded on cpu6 (Phytium FTC663)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       2407   100   2342   2342  |      28638   100   2445   2445
23:       2232   100   2275   2275  |      27927   100   2418   2417
24:       2129   100   2290   2290  |      27666   100   2429   2429
25:       2077   100   2372   2372  |      27213   100   2422   2422
----------------------------------  | ------------------------------
Avr:             100   2320   2320  |              100   2429   2428
Tot:             100   2374   2374

##########################################################################

Executing benchmark 3 times multi-threaded

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:   1765 MB,  # Benchmark threads:      8

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:      15165   661   2233  14753  |     214898   774   2368  18330
23:      14622   659   2262  14899  |     209434   770   2355  18124
24:      14845   674   2368  15962  |     209935   782   2358  18426
25:      14217   723   2245  16233  |     206951   784   2349  18418
----------------------------------  | ------------------------------
Avr:             679   2277  15462  |              777   2357  18324
Tot:             728   2317  16893

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - - 64000000 - - 256000000 - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:   1765 MB,  # Benchmark threads:      8

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:      15176   654   2259  14764  |     217628   779   2382  18563
23:      14809   660   2287  15089  |     215442   786   2373  18644
24:      14799   681   2338  15912  |     210525   784   2357  18477
25:      14514   715   2318  16572  |     205768   780   2348  18313
----------------------------------  | ------------------------------
Avr:             677   2301  15584  |              782   2365  18499
Tot:             730   2333  17042

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,8 CPUs LE)

LE
CPU Freq: - - - - 128000000 - - - -

RAM size:   15879 MB,  # CPU hardware threads:   8
RAM usage:   1765 MB,  # Benchmark threads:      8

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:      15081   662   2216  14671  |     218979   789   2369  18678
23:      14771   656   2294  15050  |     214653   787   2361  18575
24:      14721   669   2367  15829  |     209257   782   2349  18366
25:      14370   721   2277  16408  |     205094   780   2341  18253
----------------------------------  | ------------------------------
Avr:             677   2288  15490  |              784   2355  18468
Tot:             731   2322  16979

Compression: 15462,15584,15490
Decompression: 18324,18499,18468
Total: 16893,17042,16979

##########################################################################

Testing clockspeeds again. System health now:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
00:53:45: 2300/2300MHz 10.02  96%   0%  95%   0%   0%   0%     0Â°C

Checking cpufreq OPP for cpu0-cpu1 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 2280 (2275.234/2298.719/2297.385)
Cpufreq OPP: 1150    Measured: 1150 (1147.773/1146.041/1146.830)
Cpufreq OPP:  766    Measured:  665 (661.545/677.762/669.204)
Cpufreq OPP:  575    Measured:  575 (572.583/528.938/572.626)

Checking cpufreq OPP for cpu2-cpu3 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 2300 (2297.123/2298.300/2299.217)
Cpufreq OPP: 1150    Measured:  490 (486.909/487.005/524.712)
Cpufreq OPP:  766    Measured:  360 (359.033/766.258/764.878)
Cpufreq OPP:  575    Measured:  250 (246.698/344.868/246.728)

Checking cpufreq OPP for cpu4-cpu5 (Phytium FTC663):

Cpufreq OPP: 2300    Measured:  950 (947.986/1342.173/1342.330)
Cpufreq OPP: 1150    Measured: 1150 (1146.156/1148.066/1148.679)
Cpufreq OPP:  766    Measured:  770 (765.146/764.271/742.303)
Cpufreq OPP:  575    Measured:  575 (574.035/574.035/573.738)

Checking cpufreq OPP for cpu6-cpu7 (Phytium FTC663):

Cpufreq OPP: 2300    Measured: 1090 (1087.795/1476.649/861.084)
Cpufreq OPP: 1150    Measured:  455 (454.491/406.735/524.926)
Cpufreq OPP:  766    Measured:  320 (317.080/317.098/317.011)
Cpufreq OPP:  575    Measured:  575 (571.698/573.944/573.832)

##########################################################################

Hardware sensors:

scpi_sensors-isa-0000
:             +58.6 C  
:             +54.0 C  

amdgpu-pci-0400
fan1:           0 RPM
edge:         +57.0 C  (crit = +120.0 C, hyst = +90.0 C)

##########################################################################

System health while running tinymembench:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
00:19:53: 2300/2300MHz  3.36   9%   3%   4%   0%   1%   0%    --
00:22:33: 2300/2300MHz  3.96  12%   0%  12%   0%   0%   0%    --
00:25:13: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --
00:27:53: 2300/2300MHz  4.69  23%   0%  21%   0%   0%   0%    --
00:30:33: 2300/2300MHz  4.05  12%   0%  12%   0%   0%   0%    --
00:33:13: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --
00:35:53: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --
00:38:33: 2300/2300MHz  4.01  12%   0%  12%   0%   0%   0%    --
00:41:13: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --

System health while running OpenSSL benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
00:43:47: 2300/2300MHz  4.00   9%   3%   4%   0%   1%   0%    --
00:44:03: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --
00:44:19: 2300/2300MHz  4.07  12%   0%  12%   0%   0%   0%    --
00:44:35: 2300/2300MHz  4.06  12%   0%  12%   0%   0%   0%    --
00:44:51: 2300/2300MHz  4.04  12%   0%  12%   0%   0%   0%    --
00:45:07: 2300/2300MHz  4.03  12%   0%  12%   0%   0%   0%    --
00:45:23: 2300/2300MHz  4.02  12%   0%  12%   0%   0%   0%    --
00:45:39: 2300/2300MHz  4.02  12%   0%  12%   0%   0%   0%    --
00:45:55: 2300/2300MHz  4.01  12%   0%  12%   0%   0%   0%    --
00:46:11: 2300/2300MHz  4.01  12%   0%  12%   0%   0%   0%    --
00:46:27: 2300/2300MHz  4.01  12%   0%  12%   0%   0%   0%    --
00:46:43: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --
00:46:59: 2300/2300MHz  4.00  12%   0%  12%   0%   0%   0%    --
00:47:15: 2300/2300MHz  4.14  12%   0%  12%   0%   0%   0%    --

System health while running 7-zip single core benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
00:47:23: 2300/2300MHz  4.13   9%   3%   4%   0%   1%   0%    --
00:47:37: 2300/2300MHz  4.10  12%   0%  12%   0%   0%   0%    --
00:47:51: 2300/2300MHz  4.08  12%   0%  12%   0%   0%   0%    --
00:48:05: 2300/2300MHz  4.06  12%   0%  12%   0%   0%   0%    --
00:48:19: 2300/2300MHz  4.04  12%   0%  12%   0%   0%   0%    --
00:48:33: 2300/2300MHz  4.04  12%   0%  12%   0%   0%   0%    --
00:48:47: 2300/2300MHz  4.03  12%   0%  12%   0%   0%   0%    --
00:49:01: 2300/2300MHz  4.02  12%   0%  12%   0%   0%   0%    --
00:49:15: 2300/2300MHz  4.02  12%   0%  12%   0%   0%   0%    --
00:49:29: 2300/2300MHz  4.01  12%   0%  12%   0%   0%   0%    --
00:49:43: 2300/2300MHz  4.09  12%   0%  12%   0%   0%   0%    --
00:49:57: 2300/2300MHz  4.07  12%   0%  12%   0%   0%   0%    --
00:50:11: 2300/2300MHz  4.05  12%   0%  12%   0%   0%   0%    --
00:50:25: 2300/2300MHz  4.04  12%   0%  12%   0%   0%   0%    --
00:50:39: 2300/2300MHz  4.03  14%   0%  13%   0%   0%   0%    --
00:50:53: 2300/2300MHz  4.03  13%   0%  12%   0%   0%   0%    --
00:51:07: 2300/2300MHz  4.02  13%   0%  12%   0%   0%   0%    --

System health while running 7-zip multi core benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
00:51:18: 2300/2300MHz  4.02   9%   3%   4%   0%   1%   0%    --
00:51:29: 2300/2300MHz  5.09  93%   0%  93%   0%   0%   0%    --
00:51:39: 2300/2300MHz  6.47  92%   0%  91%   0%   0%   0%    --
00:51:49: 2300/2300MHz  7.02  86%   0%  85%   0%   0%   0%    --
00:52:01: 2300/2300MHz  7.48  82%   1%  81%   0%   0%   0%    --
00:52:11: 2300/2300MHz  8.10  92%   0%  92%   0%   0%   0%    --
00:52:21: 2300/2300MHz  7.99  91%   0%  91%   0%   0%   0%    --
00:52:31: 2300/2300MHz  7.89  89%   0%  89%   0%   0%   0%    --
00:52:41: 2300/2300MHz  8.37  91%   0%  91%   0%   0%   0%    --
00:52:52: 2300/2300MHz  8.63  83%   1%  82%   0%   0%   0%    --
00:53:03: 2300/2300MHz  8.83  92%   0%  91%   0%   0%   0%    --
00:53:13: 2300/2300MHz  9.17  93%   0%  93%   0%   0%   0%    --
00:53:24: 2300/2300MHz  9.53  87%   0%  87%   0%   0%   0%    --
00:53:34: 2300/2300MHz  9.84  84%   0%  84%   0%   0%   0%    --
00:53:45: 2300/2300MHz 10.02  96%   0%  95%   0%   0%   0%    --

##########################################################################

Linux 5.4.18-35-generic (zqykj-K681-F223) 	03/16/22 	_aarch64_	(8 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           4.33    0.00    3.05    1.73    0.00   90.88

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
nvme0n1          55.77       418.95       422.68       141.12  553641671  558572440  186493032
sda               0.00         0.01         0.00         0.00       8163          0          0

              total        used        free      shared  buff/cache   available
Mem:           15Gi       2.1Gi       4.5Gi        55Mi       8.9Gi        13Gi
Swap:          18Gi       1.0Gi        17Gi

Filename				Type		Size	Used	Priority
/dev/nvme0n1p6                         	partition	19511292	1096192	-2

CPU sysfs topology (clusters, cpufreq members, clockspeeds)
                 cpufreq   min    max
 CPU    cluster  policy   speed  speed   core type
  0       36        0      575    2300   Phytium FTC663 / r1p3
  1       36        0      575    2300   Phytium FTC663 / r1p3
  2       36        2      575    2300   Phytium FTC663 / r1p3
  3       36        2      575    2300   Phytium FTC663 / r1p3
  4       36        4      575    2300   Phytium FTC663 / r1p3
  5       36        4      575    2300   Phytium FTC663 / r1p3
  6       36        6      575    2300   Phytium FTC663 / r1p3
  7       36        6      575    2300   Phytium FTC663 / r1p3

Architecture:                    aarch64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
CPU(s):                          8
On-line CPU(s) list:             0-7
Thread(s) per core:              1
Core(s) per socket:              8
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       0x70
Model:                           3
Model name:                      Phytium,D2000/8 E8C
Stepping:                        0x1
CPU max MHz:                     2300.0000
CPU min MHz:                     575.0000
BogoMIPS:                        96.00
L1d cache:                       256 KiB
L1i cache:                       256 KiB
L2 cache:                        8 MiB
L3 cache:                        4 MiB
NUMA node0 CPU(s):               0-7
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; __user pointer sanitization
Vulnerability Spectre v2:        Vulnerable
Vulnerability Tsx async abort:   Not affected
Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid

Signature: 360Phytiumr1p3360Phytiumr1p3362Phytiumr1p3362Phytiumr1p3364Phytiumr1p3364Phytiumr1p3366Phytiumr1p3366Phytiumr1p3
 Compiler: /usr/bin/gcc (Ubuntu 9.3.0-10kylin2/aarch64-linux-gnu)
 Userland: arm64
   Kernel: 5.4.18-35-generic/aarch64
           CONFIG_HZ=250
           CONFIG_HZ_250=y
           CONFIG_PREEMPT_VOLUNTARY=y

DIMM configuration: 
	Locator: DDR4_A
	Bank Locator: BANK1
	Type: DDR4
	Speed: 2666 MT/s
	Rank: 2
	Configured Memory Speed: 2666 MT/s
 
	Locator: DDR4_B
	Bank Locator: BANK2
	Type: DDR4
	Speed: 2666 MT/s
	Rank: 2
	Configured Memory Speed: 2666 MT/s

cpu0/index2: 2048K, level: 2, type: Unified
cpu0/index0: 32K, level: 1, type: Data
cpu0/index3: 4096K, level: 3, type: Unified
cpu0/index1: 32K, level: 1, type: Instruction
cpu1/index2: 2048K, level: 2, type: Unified
cpu1/index0: 32K, level: 1, type: Data
cpu1/index3: 4096K, level: 3, type: Unified
cpu1/index1: 32K, level: 1, type: Instruction
cpu2/index2: 2048K, level: 2, type: Unified
cpu2/index0: 32K, level: 1, type: Data
cpu2/index3: 4096K, level: 3, type: Unified
cpu2/index1: 32K, level: 1, type: Instruction
cpu3/index2: 2048K, level: 2, type: Unified
cpu3/index0: 32K, level: 1, type: Data
cpu3/index3: 4096K, level: 3, type: Unified
cpu3/index1: 32K, level: 1, type: Instruction
cpu4/index2: 2048K, level: 2, type: Unified
cpu4/index0: 32K, level: 1, type: Data
cpu4/index3: 4096K, level: 3, type: Unified
cpu4/index1: 32K, level: 1, type: Instruction
cpu5/index2: 2048K, level: 2, type: Unified
cpu5/index0: 32K, level: 1, type: Data
cpu5/index3: 4096K, level: 3, type: Unified
cpu5/index1: 32K, level: 1, type: Instruction
cpu6/index2: 2048K, level: 2, type: Unified
cpu6/index0: 32K, level: 1, type: Data
cpu6/index3: 4096K, level: 3, type: Unified
cpu6/index1: 32K, level: 1, type: Instruction
cpu7/index2: 2048K, level: 2, type: Unified
cpu7/index0: 32K, level: 1, type: Data
cpu7/index3: 4096K, level: 3, type: Unified
cpu7/index1: 32K, level: 1, type: Instruction

| zqykj-K681-F223 | 2300/2300 MHz | 5.4 | Kylin arm64 | 16970 | 875630 | 1657030 | 4090 | 16290 | - |